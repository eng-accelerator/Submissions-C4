"""
Report Chat Agent
━━━━━━━━━━━━━━━━━
Seventh agent (interactive, post-pipeline). Like having a knowledgeable
analyst sitting beside you who's read the entire research report — you
ask questions, they answer from the findings.

Uses the LLM (OpenRouter) with structured context from the pipeline.

Responsibilities:
  • Answer follow-up questions about the research report
  • Reference specific claims, themes, and sources
  • Stay grounded in the report — never hallucinate
"""

from __future__ import annotations
from typing import Dict, Any, List
from llm import chat_completion, is_available as llm_available


_SYSTEM_PROMPT = """\
You are the Report Assistant for Astraeus, a multi-agent AI research platform.
A research report was just generated by a 6-agent pipeline. Your job is to answer
the user's follow-up questions about the research findings.

RULES:
1. Answer ONLY from the provided research context below. Never invent facts.
2. When referencing claims, cite the source ID (e.g. "[web_0]", "[demo_rag_001]").
3. Be concise but thorough. Use bullet points for clarity.
4. If the context doesn't contain the answer, say so honestly.
5. You may explain methodology (the 6-agent pipeline) if asked.
6. Keep a friendly, professional tone.
"""


def build_context_summary(context: Dict[str, Any]) -> str:
    """
    Build a structured text summary from the pipeline context.
    Like preparing a briefing document for the analyst.
    """
    sections: list[str] = []

    # Research query
    query = context.get("query", "")
    if query:
        sections.append(f"## RESEARCH QUERY\n{query}")

    # Report (truncated to stay within context limits)
    report = context.get("report_markdown", "")
    if report:
        sections.append(f"## RESEARCH REPORT\n{report[:5000]}")

    # Claims with verdicts
    fact_results = context.get("fact_check_results", [])
    if fact_results:
        lines = []
        for r in fact_results[:12]:
            verdict = r.get("verdict", "?")
            score = r.get("credibility_score", 0)
            src = r.get("source_id", "?")
            lines.append(
                f"- [{verdict.upper()}] (score {score:.2f}, source: {src}) "
                f"{r.get('claim', '')[:150]}"
            )
        sections.append("## FACT-CHECKED CLAIMS\n" + "\n".join(lines))

    # Themes
    themes = context.get("themes", [])
    if themes:
        lines = []
        for t in themes[:6]:
            if isinstance(t, dict):
                lines.append(f"- {t.get('theme', t.get('name', str(t)))}")
            else:
                lines.append(f"- {str(t)}")
        sections.append("## THEMES\n" + "\n".join(lines))

    # Gaps
    gaps = context.get("gaps", [])
    if gaps:
        lines = []
        for g in gaps[:6]:
            if isinstance(g, dict):
                lines.append(f"- {g.get('gap', g.get('description', str(g)))}")
            else:
                lines.append(f"- {str(g)}")
        sections.append("## RESEARCH GAPS\n" + "\n".join(lines))

    # Hypotheses
    hypotheses = context.get("hypotheses", [])
    if hypotheses:
        lines = []
        for h in hypotheses[:6]:
            if isinstance(h, dict):
                lines.append(f"- {h.get('hypothesis', h.get('text', str(h)))}")
            else:
                lines.append(f"- {str(h)}")
        sections.append("## HYPOTHESES\n" + "\n".join(lines))

    # Credibility summary
    cred = context.get("credibility_summary", {})
    if cred:
        overall = cred.get("overall", "?")
        avg = cred.get("average_score", 0)
        sections.append(
            f"## SOURCE CREDIBILITY\nOverall: {overall} (avg score: {avg:.2f})"
        )

    # Contradictions
    contradictions = context.get("contradictions", [])
    if contradictions:
        lines = []
        for c in contradictions[:4]:
            lines.append(
                f"- \"{c.get('claim_a', '')}\" vs \"{c.get('claim_b', '')}\" "
                f"({c.get('opposing_terms', '')})"
            )
        sections.append("## CONTRADICTIONS\n" + "\n".join(lines))

    return "\n\n".join(sections)


def chat(
    user_message: str,
    context: Dict[str, Any],
    chat_history: List[Dict[str, str]],
) -> str:
    """
    Send a user message to the LLM with research context and chat history.

    Args:
        user_message: The user's question
        context: Pipeline context dict (report, claims, themes, etc.)
        chat_history: Previous messages [{"role": ..., "content": ...}]

    Returns:
        The assistant's response text.
    """
    if not llm_available():
        return (
            "I'd love to help, but the LLM isn't configured yet. "
            "Please add your `OPENROUTER_API_KEY` to the `.env` file "
            "and restart the app."
        )

    research_context = build_context_summary(context)

    messages = [
        {
            "role": "system",
            "content": f"{_SYSTEM_PROMPT}\n\n---\n\n{research_context}",
        },
    ]

    # Add recent conversation history (last 10 messages for context window)
    for msg in chat_history[-10:]:
        messages.append({"role": msg["role"], "content": msg["content"]})

    # Add current user message
    messages.append({"role": "user", "content": user_message})

    response = chat_completion(
        messages=messages,
        max_tokens=800,
        temperature=0.3,
    )

    return response or "I wasn't able to generate a response. Please try again."
